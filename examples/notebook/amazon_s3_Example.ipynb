{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables to be set:\n",
    "- export PYSPARK_PYTHON=/opt/anaconda/bin/python3\n",
    "- export PYSPARK_DRIVER_PYTHON=\"jupyter\"\n",
    "- export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\n",
    "\n",
    "### Script to run at command line to enable this notebook\n",
    "- /opt/spark/bin/pyspark --master spark://spark-master-245:7077 --jars /opt/symetry/lib/sym-spark-assembly.jar,\\\n",
    "/PATH_TO_JARS/aws-java-sdk-1.7.4.jar,\\\n",
    "/PATH_TO_JARS/hadoop-aws-2.7.3.jar,\\\n",
    "/PATH_TO_JARS/jets3t-0.9.4.jar \\\n",
    "--driver-java-options -Dsym.lic.loc=/opt/symetry/sym.lic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazonS3Example.ipynb start\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "print(\"amazonS3Example.ipynb start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function copies a Java list to a Python list.\n",
    "# The converted list is totally seperated from its Java version.\n",
    "def tolist(l):\n",
    "    if type(l)==py4j.java_collections.JavaList:\n",
    "        k = []\n",
    "        for i in range(0,len(l)):\n",
    "            k.append(tolist(l[i]))\n",
    "    else:\n",
    "        k = l\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code Starts from Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ Amazon S3 Credentials from env variables\n",
    "awsAccessKeyId = \"toto\" # os.environ['AWS_ACCESS_KEY']\n",
    "awsSecretAccessKey = \"momy\" # os.environ['AWS_SECRET_KEY']\n",
    "\n",
    "# print(\"awsAccessKeyId=\" + awsAccessKeyId)\n",
    "# print(\"awsSecretAccessKey=\" + awsSecretAccessKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a CSV data file\n",
    "from pyspark.mllib.common import _py2java, _java2py\n",
    "from   py4j.java_collections import ListConverter\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",awsAccessKeyId)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",awsSecretAccessKey)\n",
    "\n",
    "myrdd  = sc.textFile(\"s3a://sml-oregon/datasets/susy/SUSYmini.csv\")\n",
    "# Convert pyspark RDD to JavaRDD\n",
    "myJavaRdd = _py2java(sc, myrdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first line of CSV file are the name of the attributes\n",
    "attributeNames = myrdd.first().split(\",\")\n",
    "# The attributeTypes has to be given\n",
    "attributeTypes = [\"B\"]+[\"C\"]*(len(attributeNames)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The IP address of the host if empty, project is not persisted. (Not Persisted Here)\n",
    "gateway         = sc._gateway\n",
    "sym             = gateway.jvm.com.sml.shell\n",
    "sym.SymShellConfig.set(\"RedisHost\",\"localhost\")\n",
    "sym.SymShellConfig.set(\"RedisPort\",6379)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.com.sml.shell.PySparkShellSymetryProject.\n: java.lang.Exception: Persistence not available for Scala / Python projects\n\tat com.sml.shell.PySparkShellSymetryProject.<init>(PySparkShell.scala:125)\n\tat com.sml.shell.PySparkShellSymetryProject.<init>(PySparkShell.scala:90)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b203440f873d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgateway\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msym\u001b[0m             \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mp\u001b[0m               \u001b[0;34m=\u001b[0m \u001b[0msym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPySparkShellSymetryProject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprojectName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojectType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1525\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.com.sml.shell.PySparkShellSymetryProject.\n: java.lang.Exception: Persistence not available for Scala / Python projects\n\tat com.sml.shell.PySparkShellSymetryProject.<init>(PySparkShell.scala:125)\n\tat com.sml.shell.PySparkShellSymetryProject.<init>(PySparkShell.scala:90)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "# 1) Create the Project here\n",
    "projectName     = \"amazonS3ExampleInNotebook\"\n",
    "userName        = \"c1\"\n",
    "projectType     =  0         # 0: Using CPU, 11:using GPU\n",
    "gateway         = sc._gateway\n",
    "sym             = gateway.jvm.com.sml.shell\n",
    "p               = sym.PySparkShellSymetryProject(userName,projectName, projectType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable Histogram\n",
    "bins = 100\n",
    "subject = \"lepton-2-pT\"\n",
    "\n",
    "p.setBuildHistogram(True, 1000);\n",
    "p.learn(sc._jsc, myJavaRdd, attributeNames, attributeTypes, None)\n",
    "density = p.getPDFForAttribute(subject, bins, False, True)\n",
    "x = density.getHistogram()\n",
    "\n",
    "sigma = math.sqrt(p.univariate(subject)['variance'])\n",
    "mu = p.univariate(subject)['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Width, Min, Max of Density\n",
    "xw = density.getWidth()\n",
    "xmin = density.getMin()\n",
    "xmax = density.getMax()\n",
    "x_width = []\n",
    "#Building x array\n",
    "for k in range(0,100):\n",
    "    xr = k*xw\n",
    "    x_width.append(xr+xmin)\n",
    "    x_list = []\n",
    "#Building y array\n",
    "for i in x:\n",
    "    x_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting of PDF\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(x_width, x_list, width = 0.04, color='#0504aa',alpha=0.5)\n",
    "plt.xlim(min(x_width), max(x_width))\n",
    "plt.grid(axis='y', alpha=0.5)\n",
    "plt.xlabel('Values',fontsize=10)\n",
    "plt.ylabel('Frequency',fontsize=15)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.ylabel('Frequency',fontsize=10)\n",
    "plt.title(subject+\" Distribution\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Learn the RDD\n",
    "# sc : is the SparkContext which is automatically generated (sc._jsc: is its Java version)\n",
    "p.learn(sc._jsc, myJavaRdd, attributeNames, attributeTypes, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3)  some data exploration (univariate, and bivariate Statistics)\n",
    "print(p.univariate(4)) # exploration , see whether the project has been built correctly\n",
    "print(p.univariate(\"lepton-2-pT\")) # You may pass the name of the attribute as well\n",
    "stats = p.univariate(\"lepton-2-pT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring some univariate statistics\n",
    "x = range(1,len(attributeNames))\n",
    "attrVariance=[p.univariate(i-1)[\"variance\"] for i in x]\n",
    "attrMean=[p.univariate(i-1)[\"mean\"] for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting Distributions\")\n",
    "l1ptv = p.univariate(\"lepton-1-pT\")['stddev']\n",
    "l1ptm = p.univariate(\"lepton-1-pT\")['mean']\n",
    "l1ev = p.univariate(\"lepton-1-eta\")['stddev']\n",
    "l1em = p.univariate(\"lepton-1-eta\")['mean']\n",
    "l1pv = p.univariate(\"lepton-1-phi\")['stddev']\n",
    "l1pm = p.univariate(\"lepton-1-phi\")['mean']\n",
    "x = ['lepton-1-pT','lepton-1-eta','lepton-1-phi']\n",
    "e = np.array([l1ptv,l1ev,l1pv])\n",
    "y = np.array([l1ptm,l1em,l1pm])\n",
    "plt.errorbar(x, y, e, linestyle='None', marker='^')\n",
    "plt.title('Distribution of characteristics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p.bivariate(\"lepton-2-pT\",\"lepton-2-phi\")[\"linCorr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the pairwise correlation coefficients\n",
    "linCorr=[]\n",
    "for attr1 in attributeNames:\n",
    "    temp = []\n",
    "    for attr2 in attributeNames:  \n",
    "        b1=p.bivariate(attr1,attr2) #calculates two bivariate statistics\n",
    "        temp.append(b1[\"linCorr\"])\n",
    "    linCorr.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(range(0,len(attributeNames)))\n",
    "plt.yticks(range(0,len(attributeNames)))\n",
    "plt.imshow(linCorr,cmap=\"hot\",interpolation='none')\n",
    "plt.title(\"Linear Correlation\")\n",
    "_ = plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Perform PCA\n",
    "e1 = p.pca(range(1,19)) # returns a Tuple[eigenvalues,eigenvectors]\n",
    "print(e1[\"EigenValues\"][0])                                     # the first eigen-values\n",
    "print(e1[\"EigenVectors\"][0])                                      # the first eigen-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = e1[\"EigenVectors\"][0:5]\n",
    "v = tolist(v)\n",
    "# in order to plot e1 ,we need to convert it to a List, we used predefined tolist function\n",
    "\n",
    "plt.xticks(range(0,len(attributeNames)))\n",
    "plt.yticks(range(0,len(attributeNames)))\n",
    "plt.imshow(v,cmap=\"hot\",interpolation='none')\n",
    "plt.ylabel(\"PCA EigenVectos\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2 = p.pca([\n",
    "    \"lepton-1-pT\",\n",
    "    \"lepton-1-eta\", \"lepton-1-phi\",\n",
    "    \"lepton-2-pT\", \"lepton-2-eta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Build model\n",
    "tar   = [0]                              # The forth Attribute is used as Target\n",
    "input = range(1,19)                      # The first four attribute used as as Input\n",
    " \n",
    "p.buildModel(input, tar, \"lsvm\", \"mySvmModel\")       # The Model is know built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One row of data (Attributes are comma separated)\n",
    "# Here in this model, the first Attribute is the Target,\n",
    "# we put an arbitrary value as it will be ignored in the prediction\n",
    "\n",
    "# The response for this example should be 1\n",
    "test1 = [\"-1\",\"1.667973\", \"0.0641906\",\"-1.2251714\",\n",
    "\"0.506102\", \"-0.3389389\", \"1.6725428\",\n",
    "\"3.475464\", \"-1.2191363\", \"0.0129545\",\n",
    "\"3.775173\", \"1.0459771\",  \"0.5680512\",\n",
    "\"0.481928\", \"0.0000000\", \"0.4484102\",\n",
    "\"0.205355\", \"1.3218934\", \"0.3775840\"]\n",
    "\n",
    "df1 = sym.PyDataFrame() # The dataframe has to be type PyDataFrame\n",
    "df1.setAttributeNames(attributeNames)\n",
    "df1.setAttributeTypes(attributeTypes)\n",
    "df1.addTuple(test1)\n",
    "results1 = p.predict(df1,\"mySvmModel\")\n",
    "print(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The response for this example should be 0\n",
    "test2 = [\"-1\",\"1.001869\",\"-0.471788\",\"0.555614\",\n",
    "\"1.233368\",\"1.255548\",\"-1.052491\",\n",
    "\"0.437615\",\"-1.333052\",\"0.326858\",\n",
    "\"-0.111678\",\"1.435708\",\"0.755201\",\n",
    "\"0.466779\",\"0.454541\",\"1.446331\",\n",
    "\"0.592259\",\"1.325197\",\"0.083014\"]\n",
    "\n",
    "df1.clear()\n",
    "df1.addTuple(test2)\n",
    "results2 = p.predict(df1,\"mySvmModel\")\n",
    "print(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6) You can delete the Model to release the used memory\n",
    "p.deleteModel(\"mySvmModel\")\n",
    "\n",
    "# STEP 7) Tou can delete the Project to release the used memory\n",
    "p.deleteProject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"amazonS3Example.ipynb end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
